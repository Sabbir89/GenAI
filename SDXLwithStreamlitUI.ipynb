{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''!pip install invisible_watermark accelerate safetensors\n",
        "!pip install transformers\n",
        "!pip install diffusers\n",
        "!pip install streamlit\n",
        "!pip install openai\n",
        "!pip install pyngrok '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unHgOYXx28iN",
        "outputId": "6bc9feb4-7d3b-4c6e-b18a-2a5274566fc9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting invisible_watermark\n",
            "  Downloading invisible_watermark-0.2.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.33.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.4.4)\n",
            "Requirement already satisfied: Pillow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from invisible_watermark) (9.4.0)\n",
            "Collecting PyWavelets>=1.1.1 (from invisible_watermark)\n",
            "  Downloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from invisible_watermark) (1.26.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from invisible_watermark) (4.10.0.84)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from invisible_watermark) (2.4.0+cu121)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.24.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->invisible_watermark) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->invisible_watermark) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->invisible_watermark) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->invisible_watermark) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->invisible_watermark) (1.3.0)\n",
            "Downloading invisible_watermark-0.2.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pywavelets-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets, invisible_watermark\n",
            "Successfully installed PyWavelets-1.7.0 invisible_watermark-0.2.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Collecting diffusers\n",
            "  Downloading diffusers-0.30.2-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.24.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (9.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.8.30)\n",
            "Downloading diffusers-0.30.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diffusers\n",
            "Successfully installed diffusers-0.30.2\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.38.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.8.0)\n",
            "Collecting tenacity<9,>=8.1.0 (from streamlit)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<5,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl.metadata (38 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.38.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: watchdog, tenacity, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.38.0 tenacity-8.5.0 watchdog-4.0.2\n",
            "Collecting openai\n",
            "  Downloading openai-1.44.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.44.0-py3-none-any.whl (367 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.8/367.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.44.0\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Already, we have the python file. As we are going to use Google Colab, that's why we have to upload the python file (app.py) here to proceed with the streamlit application. Also, we must install all the dependencies to move ahead."
      ],
      "metadata": {
        "id": "yRoYW7_SzeY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "1Z5YOGC2k1WG",
        "outputId": "e6b9cef4-3abb-4e3b-b0ec-84f2e187dfad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1da196b1-10c4-4039-a8e9-9e214e18ea08\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1da196b1-10c4-4039-a8e9-9e214e18ea08\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving app.py to app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To have the streamlit's UI, we have to use ngrok and for that we need to have a authtoken from the website: https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "\n",
        "To have the token, first we have to sign up to the site."
      ],
      "metadata": {
        "id": "ulPpGslF13ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "import subprocess"
      ],
      "metadata": {
        "id": "A6z5aa0e3kpT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2lLbLGmDASWtgvw5DXbxkElD36v_5vQM7a3F1123DniAVC1wt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfvTTpZDwKYM",
        "outputId": "afa876f1-6d8a-4d6e-a294-2d8074134974"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Kj5upbox2Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup a tunnel to the streamlit port 8501\n",
        "\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ec34e1f-e7a1-4bab-d569-008bf92b9ea0",
        "id": "tSKVFZqrx6Gg"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NgrokTunnel: \"https://2ad3-34-45-134-246.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Click on the link \"https://2ad3-34-45-134-246.ngrok-free.app\" to open in it on your browser after running the below command (!streamlit run app.py)"
      ],
      "metadata": {
        "id": "_8YryVXZ5giF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlG-6SQ80C4o",
        "outputId": "b2de88d3-2bf1-4b16-f944-9883b9a7f7ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.45.134.246:8501\u001b[0m\n",
            "\u001b[0m\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]\n",
            "2024-09-08 13:43:25.007939: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-09-08 13:43:25.028302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-08 13:43:25.054454: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-08 13:43:25.062394: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-08 13:43:25.080928: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-08 13:43:26.256560: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "model_index.json: 100% 609/609 [00:00<00:00, 4.22MB/s]\n",
            "Fetching 19 files:   0% 0/19 [00:00<?, ?it/s]\n",
            "text_encoder_2/config.json: 100% 575/575 [00:00<00:00, 4.05MB/s]\n",
            "\n",
            "tokenizer/special_tokens_map.json: 100% 472/472 [00:00<00:00, 4.28MB/s]\n",
            "\n",
            "tokenizer/tokenizer_config.json: 100% 737/737 [00:00<00:00, 6.58MB/s]\n",
            "\n",
            "tokenizer/merges.txt:   0% 0.00/525k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "text_encoder/config.json: 100% 565/565 [00:00<00:00, 4.90MB/s]\n",
            "\n",
            "\n",
            "model.fp16.safetensors:   0% 0.00/246M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:   0% 0.00/1.39G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "scheduler/scheduler_config.json: 100% 479/479 [00:00<00:00, 3.52MB/s]\n",
            "tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 10.0MB/s]\n",
            "\n",
            "tokenizer/vocab.json:   0% 0.00/1.06M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_2/special_tokens_map.json: 100% 460/460 [00:00<00:00, 2.87MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_2/tokenizer_config.json: 100% 725/725 [00:00<00:00, 5.95MB/s]\n",
            "\n",
            "\n",
            "\n",
            "model.fp16.safetensors:   1% 10.5M/1.39G [00:00<00:15, 90.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "unet/config.json: 100% 1.68k/1.68k [00:00<00:00, 14.8MB/s]\n",
            "\n",
            "\n",
            "tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 12.3MB/s]\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   0% 0.00/5.14G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vae/config.json: 100% 642/642 [00:00<00:00, 4.84MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   0% 0.00/167M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:   2% 31.5M/1.39G [00:00<00:09, 146MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   0% 0.00/167M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.fp16.safetensors:  13% 31.5M/246M [00:00<00:01, 129MB/s] \u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:   0% 10.5M/5.14G [00:00<00:58, 87.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   6% 10.5M/167M [00:00<00:02, 77.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:   4% 52.4M/1.39G [00:00<00:07, 173MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   6% 10.5M/167M [00:00<00:02, 77.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.fp16.safetensors:  21% 52.4M/246M [00:00<00:01, 139MB/s]\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:   1% 31.5M/5.14G [00:00<00:41, 123MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:   5% 73.4M/1.39G [00:00<00:07, 170MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  19% 31.5M/167M [00:00<00:01, 120MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  19% 31.5M/167M [00:00<00:00, 144MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.fp16.safetensors:  30% 73.4M/246M [00:00<00:01, 155MB/s]\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:   1% 62.9M/5.14G [00:00<00:30, 167MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  31% 52.4M/167M [00:00<00:00, 144MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:   8% 105M/1.39G [00:00<00:06, 193MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  38% 62.9M/167M [00:00<00:00, 187MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.fp16.safetensors:  38% 94.4M/246M [00:00<00:00, 161MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  44% 73.4M/167M [00:00<00:00, 156MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:   2% 94.4M/5.14G [00:00<00:26, 188MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  10% 136M/1.39G [00:00<00:06, 208MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  56% 94.4M/167M [00:00<00:00, 209MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.fp16.safetensors:  47% 115M/246M [00:00<00:00, 164MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  56% 94.4M/167M [00:00<00:00, 162MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:   2% 126M/5.14G [00:00<00:24, 207MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  12% 168M/1.39G [00:00<00:05, 219MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  75% 126M/167M [00:00<00:00, 219MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.fp16.safetensors:  55% 136M/246M [00:00<00:00, 165MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  69% 115M/167M [00:00<00:00, 166MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:   3% 157M/5.14G [00:00<00:22, 220MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  14% 199M/1.39G [00:00<00:05, 223MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.fp16.safetensors:  64% 157M/246M [00:01<00:00, 168MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  94% 157M/167M [00:00<00:00, 219MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors: 100% 167M/167M [00:00<00:00, 201MB/s]\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  72% 178M/246M [00:01<00:00, 169MB/s]\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:   4% 189M/5.14G [00:00<00:23, 214MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  17% 231M/1.39G [00:01<00:05, 220MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  94% 157M/167M [00:00<00:00, 169MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors: 100% 167M/167M [00:01<00:00, 157MB/s]\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   4% 220M/5.14G [00:01<00:21, 224MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  19% 262M/1.39G [00:01<00:04, 228MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.fp16.safetensors:  89% 220M/246M [00:01<00:00, 179MB/s]\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:   5% 252M/5.14G [00:01<00:21, 231MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  21% 294M/1.39G [00:01<00:04, 235MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.fp16.safetensors:  98% 241M/246M [00:01<00:00, 177MB/s]\u001b[A\u001b[A\n",
            "model.fp16.safetensors: 100% 246M/246M [00:01<00:00, 163MB/s]\n",
            "Fetching 19 files:  21% 4/19 [00:01<00:07,  2.11it/s]\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  23% 325M/1.39G [00:01<00:04, 242MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:   6% 315M/5.14G [00:01<00:19, 249MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  26% 357M/1.39G [00:01<00:04, 247MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:   7% 346M/5.14G [00:01<00:19, 251MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  28% 388M/1.39G [00:01<00:03, 251MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:   7% 377M/5.14G [00:01<00:18, 255MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  30% 419M/1.39G [00:01<00:03, 254MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:   8% 409M/5.14G [00:01<00:18, 258MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  32% 451M/1.39G [00:01<00:03, 257MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:   9% 440M/5.14G [00:01<00:17, 262MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  35% 482M/1.39G [00:02<00:03, 258MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:   9% 472M/5.14G [00:02<00:17, 263MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  37% 514M/1.39G [00:02<00:03, 258MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  10% 503M/5.14G [00:02<00:17, 263MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  39% 545M/1.39G [00:02<00:03, 257MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  10% 535M/5.14G [00:02<00:17, 262MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  42% 577M/1.39G [00:02<00:03, 256MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  11% 566M/5.14G [00:02<00:17, 262MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  44% 608M/1.39G [00:02<00:03, 254MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  12% 598M/5.14G [00:02<00:17, 257MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  46% 640M/1.39G [00:02<00:02, 260MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  12% 629M/5.14G [00:02<00:17, 256MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  48% 671M/1.39G [00:02<00:02, 262MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  13% 661M/5.14G [00:02<00:17, 258MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  51% 703M/1.39G [00:02<00:02, 263MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  13% 692M/5.14G [00:02<00:17, 258MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  53% 734M/1.39G [00:03<00:02, 262MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  14% 724M/5.14G [00:03<00:17, 257MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  55% 765M/1.39G [00:03<00:02, 264MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  15% 755M/5.14G [00:03<00:16, 260MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  57% 797M/1.39G [00:03<00:02, 261MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  15% 786M/5.14G [00:03<00:19, 226MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  60% 828M/1.39G [00:03<00:02, 222MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  16% 818M/5.14G [00:03<00:18, 234MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  62% 860M/1.39G [00:03<00:02, 233MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  17% 849M/5.14G [00:03<00:17, 245MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  64% 891M/1.39G [00:03<00:02, 243MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  17% 881M/5.14G [00:03<00:17, 248MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  66% 923M/1.39G [00:03<00:01, 249MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  18% 912M/5.14G [00:03<00:16, 255MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  69% 954M/1.39G [00:03<00:01, 253MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  18% 944M/5.14G [00:03<00:16, 257MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  71% 986M/1.39G [00:04<00:01, 256MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  19% 975M/5.14G [00:04<00:16, 259MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  73% 1.02G/1.39G [00:04<00:01, 259MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  20% 1.01G/5.14G [00:04<00:16, 257MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  75% 1.05G/1.39G [00:04<00:01, 260MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  20% 1.04G/5.14G [00:04<00:15, 261MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  78% 1.08G/1.39G [00:04<00:01, 260MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  21% 1.07G/5.14G [00:04<00:15, 262MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  80% 1.11G/1.39G [00:04<00:01, 263MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  21% 1.10G/5.14G [00:04<00:15, 260MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  82% 1.14G/1.39G [00:04<00:00, 267MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  22% 1.13G/5.14G [00:04<00:15, 262MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  85% 1.17G/1.39G [00:04<00:00, 267MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  23% 1.16G/5.14G [00:04<00:15, 262MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  87% 1.21G/1.39G [00:04<00:00, 266MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  23% 1.20G/5.14G [00:04<00:15, 261MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  89% 1.24G/1.39G [00:05<00:00, 268MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  91% 1.27G/1.39G [00:05<00:00, 266MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  24% 1.23G/5.14G [00:05<00:15, 257MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  94% 1.30G/1.39G [00:05<00:00, 247MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  25% 1.26G/5.14G [00:05<00:16, 232MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  96% 1.33G/1.39G [00:05<00:00, 226MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  25% 1.29G/5.14G [00:05<00:17, 215MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors:  98% 1.36G/1.39G [00:05<00:00, 214MB/s]\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  26% 1.32G/5.14G [00:05<00:18, 206MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model.fp16.safetensors: 100% 1.39G/1.39G [00:06<00:00, 230MB/s]\n",
            "Fetching 19 files:  32% 6/19 [00:06<00:16,  1.29s/it]\n",
            "diffusion_pytorch_model.fp16.safetensors:  26% 1.35G/5.14G [00:05<00:28, 134MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  27% 1.38G/5.14G [00:06<00:23, 158MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  28% 1.42G/5.14G [00:06<00:21, 177MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  28% 1.45G/5.14G [00:06<00:18, 196MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  29% 1.48G/5.14G [00:06<00:17, 213MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  29% 1.51G/5.14G [00:06<00:16, 225MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  30% 1.54G/5.14G [00:06<00:15, 237MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  31% 1.57G/5.14G [00:06<00:14, 246MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  31% 1.60G/5.14G [00:06<00:14, 251MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  32% 1.64G/5.14G [00:07<00:13, 255MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  32% 1.67G/5.14G [00:07<00:13, 256MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  33% 1.70G/5.14G [00:07<00:13, 260MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  34% 1.73G/5.14G [00:07<00:13, 260MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  34% 1.76G/5.14G [00:07<00:12, 262MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  35% 1.79G/5.14G [00:07<00:12, 263MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  36% 1.82G/5.14G [00:07<00:12, 262MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  36% 1.86G/5.14G [00:07<00:12, 258MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  37% 1.89G/5.14G [00:07<00:12, 260MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  37% 1.92G/5.14G [00:08<00:12, 262MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  38% 1.95G/5.14G [00:08<00:12, 262MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  39% 1.98G/5.14G [00:08<00:12, 263MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  39% 2.01G/5.14G [00:08<00:12, 258MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  40% 2.04G/5.14G [00:08<00:11, 261MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  40% 2.08G/5.14G [00:08<00:11, 261MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  41% 2.11G/5.14G [00:08<00:11, 263MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  42% 2.14G/5.14G [00:08<00:11, 263MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  42% 2.17G/5.14G [00:09<00:11, 266MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  43% 2.20G/5.14G [00:09<00:11, 265MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  43% 2.23G/5.14G [00:09<00:10, 271MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  44% 2.26G/5.14G [00:09<00:10, 274MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  45% 2.30G/5.14G [00:09<00:10, 273MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  45% 2.33G/5.14G [00:09<00:10, 275MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  46% 2.36G/5.14G [00:09<00:10, 276MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  47% 2.39G/5.14G [00:09<00:09, 276MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  47% 2.42G/5.14G [00:09<00:09, 278MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  48% 2.45G/5.14G [00:10<00:09, 281MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  48% 2.49G/5.14G [00:10<00:09, 280MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  49% 2.52G/5.14G [00:10<00:09, 277MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  50% 2.55G/5.14G [00:10<00:10, 256MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  50% 2.58G/5.14G [00:10<00:09, 262MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  51% 2.61G/5.14G [00:10<00:09, 268MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  51% 2.64G/5.14G [00:10<00:09, 272MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  52% 2.67G/5.14G [00:10<00:08, 275MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  53% 2.71G/5.14G [00:11<00:08, 275MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  53% 2.74G/5.14G [00:11<00:08, 279MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  54% 2.77G/5.14G [00:11<00:08, 279MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  55% 2.80G/5.14G [00:11<00:08, 277MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  55% 2.83G/5.14G [00:11<00:08, 277MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  56% 2.86G/5.14G [00:11<00:08, 280MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  56% 2.89G/5.14G [00:11<00:08, 279MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  57% 2.93G/5.14G [00:11<00:07, 280MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  58% 2.96G/5.14G [00:11<00:07, 278MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  58% 2.99G/5.14G [00:12<00:07, 280MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  59% 3.02G/5.14G [00:12<00:07, 274MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  59% 3.05G/5.14G [00:12<00:07, 268MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  60% 3.08G/5.14G [00:12<00:07, 270MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  61% 3.11G/5.14G [00:12<00:07, 268MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  61% 3.15G/5.14G [00:12<00:07, 266MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  62% 3.18G/5.14G [00:12<00:07, 267MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  62% 3.21G/5.14G [00:12<00:07, 268MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  63% 3.24G/5.14G [00:12<00:07, 268MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  64% 3.27G/5.14G [00:13<00:07, 263MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  64% 3.30G/5.14G [00:13<00:06, 265MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  65% 3.33G/5.14G [00:13<00:06, 262MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  66% 3.37G/5.14G [00:13<00:06, 262MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  66% 3.40G/5.14G [00:13<00:06, 265MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  67% 3.43G/5.14G [00:13<00:06, 263MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  67% 3.46G/5.14G [00:13<00:06, 259MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  68% 3.49G/5.14G [00:13<00:06, 257MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  69% 3.52G/5.14G [00:14<00:06, 258MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  69% 3.55G/5.14G [00:14<00:06, 259MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  70% 3.59G/5.14G [00:14<00:05, 260MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  70% 3.62G/5.14G [00:14<00:05, 262MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  71% 3.65G/5.14G [00:14<00:05, 261MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  72% 3.68G/5.14G [00:14<00:05, 263MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  72% 3.71G/5.14G [00:14<00:05, 267MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  73% 3.74G/5.14G [00:14<00:05, 262MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  74% 3.77G/5.14G [00:15<00:05, 263MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  74% 3.81G/5.14G [00:15<00:05, 263MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  75% 3.84G/5.14G [00:15<00:04, 265MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  75% 3.87G/5.14G [00:15<00:04, 265MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  76% 3.90G/5.14G [00:15<00:04, 264MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  77% 3.93G/5.14G [00:15<00:04, 265MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  77% 3.96G/5.14G [00:15<00:04, 263MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  78% 4.00G/5.14G [00:15<00:04, 262MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  78% 4.03G/5.14G [00:15<00:04, 262MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  79% 4.06G/5.14G [00:16<00:04, 264MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  80% 4.09G/5.14G [00:16<00:03, 266MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  80% 4.12G/5.14G [00:16<00:03, 263MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  81% 4.15G/5.14G [00:16<00:03, 264MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  81% 4.18G/5.14G [00:16<00:03, 263MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  82% 4.22G/5.14G [00:16<00:03, 264MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  83% 4.25G/5.14G [00:16<00:03, 263MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  83% 4.28G/5.14G [00:16<00:03, 260MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  84% 4.31G/5.14G [00:17<00:03, 264MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  85% 4.34G/5.14G [00:17<00:02, 265MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  85% 4.37G/5.14G [00:17<00:02, 266MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  86% 4.40G/5.14G [00:17<00:02, 263MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  86% 4.44G/5.14G [00:17<00:02, 264MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  87% 4.47G/5.14G [00:17<00:02, 268MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  88% 4.50G/5.14G [00:17<00:02, 269MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  88% 4.53G/5.14G [00:17<00:02, 269MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  89% 4.56G/5.14G [00:17<00:02, 270MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  89% 4.59G/5.14G [00:18<00:01, 272MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  90% 4.62G/5.14G [00:18<00:01, 271MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  91% 4.66G/5.14G [00:18<00:01, 273MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  91% 4.69G/5.14G [00:18<00:01, 274MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  92% 4.72G/5.14G [00:18<00:01, 271MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  93% 4.75G/5.14G [00:18<00:01, 268MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  93% 4.78G/5.14G [00:18<00:01, 270MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  94% 4.81G/5.14G [00:18<00:01, 269MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  94% 4.84G/5.14G [00:19<00:01, 270MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  95% 4.88G/5.14G [00:19<00:00, 270MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  96% 4.91G/5.14G [00:19<00:00, 266MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  96% 4.94G/5.14G [00:19<00:00, 264MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  97% 4.97G/5.14G [00:19<00:00, 262MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  97% 5.00G/5.14G [00:19<00:00, 264MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  98% 5.03G/5.14G [00:19<00:00, 267MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  99% 5.06G/5.14G [00:19<00:00, 265MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors:  99% 5.10G/5.14G [00:19<00:00, 268MB/s]\u001b[A\n",
            "diffusion_pytorch_model.fp16.safetensors: 100% 5.14G/5.14G [00:20<00:00, 255MB/s]\n",
            "Fetching 19 files: 100% 19/19 [00:20<00:00,  1.08s/it]\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  4.59it/s]\n",
            "model_index.json: 100% 612/612 [00:00<00:00, 5.69MB/s]\n",
            "Fetching 9 files:   0% 0/9 [00:00<?, ?it/s]\n",
            "scheduler/scheduler_config.json: 100% 479/479 [00:00<00:00, 4.79MB/s]\n",
            "\n",
            "tokenizer_2/tokenizer_config.json: 100% 725/725 [00:00<00:00, 5.39MB/s]\n",
            "\n",
            "tokenizer_2/vocab.json:   0% 0.00/1.06M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "unet/config.json: 100% 1.71k/1.71k [00:00<00:00, 16.5MB/s]\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   0% 0.00/167M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   0% 0.00/4.52G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_2/merges.txt:   0% 0.00/525k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_2/special_tokens_map.json: 100% 460/460 [00:00<00:00, 4.11MB/s]\n",
            "tokenizer_2/merges.txt: 100% 525k/525k [00:00<00:00, 10.2MB/s]\n",
            "Fetching 9 files:  33% 3/9 [00:00<00:00, 14.99it/s]\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   6% 10.5M/167M [00:00<00:01, 91.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   0% 10.5M/4.52G [00:00<00:52, 85.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "tokenizer_2/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 6.66MB/s]\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  25% 41.9M/167M [00:00<00:00, 195MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   1% 31.5M/4.52G [00:00<00:33, 134MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  44% 73.4M/167M [00:00<00:00, 230MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   1% 52.4M/4.52G [00:00<00:28, 158MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  63% 105M/167M [00:00<00:00, 246MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   2% 73.4M/4.52G [00:00<00:26, 165MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  81% 136M/167M [00:00<00:00, 255MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   2% 94.4M/4.52G [00:00<00:25, 174MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors: 100% 167M/167M [00:00<00:00, 254MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors: 100% 167M/167M [00:00<00:00, 235MB/s]\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   3% 136M/4.52G [00:00<00:24, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   3% 157M/4.52G [00:00<00:23, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   4% 178M/4.52G [00:01<00:23, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   4% 199M/4.52G [00:01<00:23, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   5% 220M/4.52G [00:01<00:23, 179MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   5% 241M/4.52G [00:01<00:24, 178MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   6% 262M/4.52G [00:01<00:24, 176MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   6% 283M/4.52G [00:01<00:24, 175MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   7% 304M/4.52G [00:01<00:24, 175MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   7% 325M/4.52G [00:01<00:24, 175MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   8% 346M/4.52G [00:02<00:24, 173MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   8% 367M/4.52G [00:02<00:23, 173MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   9% 388M/4.52G [00:02<00:23, 174MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:   9% 409M/4.52G [00:02<00:23, 175MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  10% 430M/4.52G [00:02<00:23, 174MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  10% 451M/4.52G [00:02<00:23, 172MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  10% 472M/4.52G [00:02<00:23, 173MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  11% 493M/4.52G [00:02<00:23, 173MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  11% 514M/4.52G [00:02<00:23, 174MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  12% 535M/4.52G [00:03<00:22, 174MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  12% 556M/4.52G [00:03<00:22, 174MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  13% 577M/4.52G [00:03<00:22, 175MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  13% 598M/4.52G [00:03<00:22, 175MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  14% 619M/4.52G [00:03<00:22, 175MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  14% 640M/4.52G [00:03<00:21, 181MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  15% 661M/4.52G [00:03<00:21, 181MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  15% 682M/4.52G [00:03<00:21, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  16% 703M/4.52G [00:04<00:20, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  16% 724M/4.52G [00:04<00:20, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  16% 744M/4.52G [00:04<00:20, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  17% 765M/4.52G [00:04<00:20, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  17% 786M/4.52G [00:04<00:20, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  18% 807M/4.52G [00:04<00:20, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  18% 828M/4.52G [00:04<00:20, 181MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  19% 849M/4.52G [00:04<00:20, 181MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  19% 870M/4.52G [00:04<00:19, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  20% 891M/4.52G [00:05<00:19, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  20% 912M/4.52G [00:05<00:19, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  21% 933M/4.52G [00:05<00:19, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  21% 954M/4.52G [00:05<00:19, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  22% 975M/4.52G [00:05<00:19, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  22% 996M/4.52G [00:05<00:19, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  23% 1.02G/4.52G [00:05<00:19, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  23% 1.04G/4.52G [00:05<00:18, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  23% 1.06G/4.52G [00:05<00:18, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  24% 1.08G/4.52G [00:06<00:18, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  24% 1.10G/4.52G [00:06<00:18, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  25% 1.12G/4.52G [00:06<00:18, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  25% 1.14G/4.52G [00:06<00:18, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  26% 1.16G/4.52G [00:06<00:18, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  26% 1.18G/4.52G [00:06<00:18, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  27% 1.21G/4.52G [00:06<00:17, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  27% 1.23G/4.52G [00:06<00:17, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  28% 1.25G/4.52G [00:06<00:17, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  28% 1.27G/4.52G [00:07<00:16, 192MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  29% 1.29G/4.52G [00:07<00:16, 192MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  29% 1.31G/4.52G [00:07<00:16, 190MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  29% 1.33G/4.52G [00:07<00:16, 189MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  30% 1.35G/4.52G [00:07<00:16, 187MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  30% 1.37G/4.52G [00:07<00:16, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  31% 1.39G/4.52G [00:07<00:17, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  31% 1.42G/4.52G [00:07<00:17, 179MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  32% 1.44G/4.52G [00:07<00:17, 181MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  32% 1.46G/4.52G [00:08<00:16, 181MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  33% 1.48G/4.52G [00:08<00:16, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  33% 1.50G/4.52G [00:08<00:16, 179MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  34% 1.52G/4.52G [00:08<00:16, 178MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  34% 1.54G/4.52G [00:08<00:16, 177MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  35% 1.56G/4.52G [00:08<00:16, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  35% 1.58G/4.52G [00:08<00:16, 179MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  36% 1.60G/4.52G [00:08<00:16, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  36% 1.63G/4.52G [00:09<00:16, 179MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  36% 1.65G/4.52G [00:09<00:16, 179MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  37% 1.67G/4.52G [00:09<00:15, 178MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  37% 1.69G/4.52G [00:09<00:15, 179MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  38% 1.71G/4.52G [00:09<00:15, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  38% 1.73G/4.52G [00:09<00:15, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  39% 1.75G/4.52G [00:09<00:15, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  39% 1.77G/4.52G [00:09<00:14, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  40% 1.79G/4.52G [00:09<00:14, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  40% 1.81G/4.52G [00:10<00:14, 187MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  41% 1.84G/4.52G [00:10<00:14, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  41% 1.86G/4.52G [00:10<00:14, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  42% 1.88G/4.52G [00:10<00:14, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  42% 1.90G/4.52G [00:10<00:13, 189MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  42% 1.92G/4.52G [00:10<00:13, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  43% 1.94G/4.52G [00:10<00:13, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  43% 1.96G/4.52G [00:10<00:13, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  44% 1.98G/4.52G [00:10<00:13, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  44% 2.00G/4.52G [00:11<00:13, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  45% 2.02G/4.52G [00:11<00:13, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  45% 2.04G/4.52G [00:11<00:13, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  46% 2.07G/4.52G [00:11<00:13, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  46% 2.09G/4.52G [00:11<00:12, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  47% 2.11G/4.52G [00:11<00:12, 190MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  47% 2.13G/4.52G [00:11<00:12, 191MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  48% 2.15G/4.52G [00:11<00:12, 192MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  48% 2.17G/4.52G [00:11<00:12, 190MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  48% 2.19G/4.52G [00:12<00:12, 189MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  49% 2.21G/4.52G [00:12<00:12, 191MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  49% 2.23G/4.52G [00:12<00:11, 192MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  50% 2.25G/4.52G [00:12<00:11, 193MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  50% 2.28G/4.52G [00:12<00:11, 191MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  51% 2.30G/4.52G [00:12<00:11, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  51% 2.32G/4.52G [00:12<00:11, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  52% 2.34G/4.52G [00:12<00:11, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  52% 2.36G/4.52G [00:12<00:11, 189MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  53% 2.38G/4.52G [00:13<00:11, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  53% 2.40G/4.52G [00:13<00:11, 187MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  54% 2.42G/4.52G [00:13<00:11, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  54% 2.44G/4.52G [00:13<00:11, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  55% 2.46G/4.52G [00:13<00:11, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  55% 2.49G/4.52G [00:13<00:11, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  55% 2.51G/4.52G [00:13<00:11, 181MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  56% 2.53G/4.52G [00:13<00:11, 181MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  56% 2.55G/4.52G [00:14<00:11, 179MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  57% 2.57G/4.52G [00:14<00:10, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  57% 2.59G/4.52G [00:14<00:10, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  58% 2.61G/4.52G [00:14<00:10, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  58% 2.63G/4.52G [00:14<00:10, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  59% 2.65G/4.52G [00:14<00:10, 179MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  59% 2.67G/4.52G [00:14<00:10, 177MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  60% 2.69G/4.52G [00:14<00:10, 177MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  60% 2.72G/4.52G [00:14<00:10, 176MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  61% 2.74G/4.52G [00:15<00:10, 175MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  61% 2.76G/4.52G [00:15<00:10, 174MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  61% 2.78G/4.52G [00:15<00:09, 175MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  62% 2.80G/4.52G [00:15<00:09, 177MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  62% 2.82G/4.52G [00:15<00:09, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  63% 2.84G/4.52G [00:15<00:09, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  63% 2.86G/4.52G [00:15<00:09, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  64% 2.88G/4.52G [00:15<00:08, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  64% 2.90G/4.52G [00:15<00:08, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  65% 2.93G/4.52G [00:16<00:08, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  65% 2.95G/4.52G [00:16<00:08, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  66% 2.97G/4.52G [00:16<00:08, 181MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  66% 2.99G/4.52G [00:16<00:08, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  67% 3.01G/4.52G [00:16<00:08, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  67% 3.03G/4.52G [00:16<00:08, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  68% 3.05G/4.52G [00:16<00:07, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  68% 3.07G/4.52G [00:16<00:07, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  68% 3.09G/4.52G [00:17<00:07, 189MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  69% 3.11G/4.52G [00:17<00:07, 191MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  69% 3.14G/4.52G [00:17<00:07, 189MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  70% 3.16G/4.52G [00:17<00:07, 190MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  70% 3.18G/4.52G [00:17<00:07, 187MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  71% 3.20G/4.52G [00:17<00:07, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  71% 3.22G/4.52G [00:17<00:07, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  72% 3.24G/4.52G [00:17<00:06, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  72% 3.26G/4.52G [00:17<00:06, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  73% 3.28G/4.52G [00:18<00:06, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  73% 3.30G/4.52G [00:18<00:06, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  74% 3.32G/4.52G [00:18<00:06, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  74% 3.34G/4.52G [00:18<00:06, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  74% 3.37G/4.52G [00:18<00:06, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  75% 3.39G/4.52G [00:18<00:06, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  75% 3.41G/4.52G [00:18<00:05, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fetching 9 files:  33% 3/9 [00:18<00:00, 14.99it/s]\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  76% 3.45G/4.52G [00:18<00:05, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  77% 3.47G/4.52G [00:19<00:05, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  77% 3.49G/4.52G [00:19<00:05, 181MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  78% 3.51G/4.52G [00:19<00:05, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  78% 3.53G/4.52G [00:19<00:05, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  79% 3.55G/4.52G [00:19<00:05, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  79% 3.58G/4.52G [00:19<00:05, 187MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  80% 3.60G/4.52G [00:19<00:04, 187MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  80% 3.62G/4.52G [00:19<00:04, 190MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  81% 3.64G/4.52G [00:19<00:04, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  81% 3.66G/4.52G [00:20<00:04, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  81% 3.68G/4.52G [00:20<00:04, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  82% 3.70G/4.52G [00:20<00:04, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  82% 3.72G/4.52G [00:20<00:04, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  83% 3.74G/4.52G [00:20<00:04, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  83% 3.76G/4.52G [00:20<00:03, 189MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  84% 3.79G/4.52G [00:20<00:03, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  84% 3.81G/4.52G [00:20<00:03, 187MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  85% 3.83G/4.52G [00:20<00:03, 189MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  85% 3.85G/4.52G [00:21<00:03, 189MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  86% 3.87G/4.52G [00:21<00:03, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  86% 3.89G/4.52G [00:21<00:03, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  87% 3.91G/4.52G [00:21<00:03, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  87% 3.93G/4.52G [00:21<00:03, 177MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  87% 3.95G/4.52G [00:21<00:03, 174MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  88% 3.97G/4.52G [00:21<00:03, 174MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  88% 4.00G/4.52G [00:21<00:03, 173MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  89% 4.02G/4.52G [00:22<00:02, 172MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  89% 4.04G/4.52G [00:22<00:02, 171MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  90% 4.06G/4.52G [00:22<00:02, 172MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  90% 4.08G/4.52G [00:22<00:02, 173MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  91% 4.10G/4.52G [00:22<00:02, 173MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  91% 4.12G/4.52G [00:22<00:02, 174MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  92% 4.14G/4.52G [00:22<00:02, 174MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  92% 4.16G/4.52G [00:22<00:02, 176MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  93% 4.18G/4.52G [00:22<00:01, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  93% 4.20G/4.52G [00:23<00:01, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  94% 4.23G/4.52G [00:23<00:01, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  94% 4.25G/4.52G [00:23<00:01, 187MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  94% 4.27G/4.52G [00:23<00:01, 191MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  95% 4.29G/4.52G [00:23<00:01, 191MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  95% 4.31G/4.52G [00:23<00:01, 192MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  96% 4.33G/4.52G [00:23<00:00, 193MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  96% 4.35G/4.52G [00:23<00:00, 193MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  97% 4.37G/4.52G [00:23<00:00, 192MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  97% 4.39G/4.52G [00:24<00:00, 192MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  98% 4.41G/4.52G [00:24<00:00, 190MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  98% 4.44G/4.52G [00:24<00:00, 188MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  99% 4.46G/4.52G [00:24<00:00, 192MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors:  99% 4.48G/4.52G [00:24<00:00, 192MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors: 100% 4.50G/4.52G [00:24<00:00, 187MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.fp16.safetensors: 100% 4.52G/4.52G [00:24<00:00, 182MB/s]\n",
            "Fetching 9 files: 100% 9/9 [00:24<00:00,  2.77s/it]\n",
            "Loading pipeline components...: 100% 5/5 [00:00<00:00, 12.83it/s]\n",
            "config.json: 100% 570/570 [00:00<00:00, 5.04MB/s]\n",
            "model.safetensors: 100% 440M/440M [00:02<00:00, 182MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 306kB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 5.16MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 6.88MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  5.11it/s]\n",
            "Loading pipeline components...: 100% 5/5 [00:00<00:00, 14.34it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "Refined Prompt (literal): A majestic lion jumping from a big stone at night\n",
            "100% 32/32 [00:04<00:00,  7.14it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/diffusers/configuration_utils.py:140: FutureWarning: Accessing config attribute `vae_latent_channels` directly via 'VaeImageProcessor' object attribute is deprecated. Please access 'vae_latent_channels' over 'VaeImageProcessor's config object instead, e.g. 'scheduler.config.vae_latent_channels'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
            "100% 8/8 [00:00<00:00,  9.50it/s]\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\n",
            "Loading pipeline components...:  29% 2/7 [00:00<00:02,  2.05it/s]\n",
            "Loading pipeline components...:  29% 2/7 [00:00<00:02,  2.22it/s]\u001b[A\n",
            "Loading pipeline components...:  57% 4/7 [00:02<00:02,  1.46it/s]\n",
            "Loading pipeline components...:  57% 4/7 [00:02<00:02,  1.49it/s]\u001b[A\n",
            "Loading pipeline components...:  71% 5/7 [00:02<00:00,  2.05it/s]\u001b[A\n",
            "Loading pipeline components...: 100% 7/7 [00:03<00:00,  1.95it/s]\n",
            "Loading pipeline components...: 100% 7/7 [00:03<00:00,  1.91it/s]\n",
            "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\n",
            "Loading pipeline components...:  60% 3/5 [00:01<00:00,  2.82it/s]\n",
            "Loading pipeline components...: 100% 5/5 [00:01<00:00,  4.69it/s]\n",
            "Loading pipeline components...: 100% 5/5 [00:00<00:00,  5.01it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.embeddings.word_embeddings.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.embeddings.position_embeddings.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.embeddings.token_type_embeddings.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.embeddings.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.embeddings.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.attention.self.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.attention.self.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.attention.self.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.attention.self.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.attention.self.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.attention.self.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.attention.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.attention.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.0.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.attention.self.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.attention.self.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.attention.self.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.attention.self.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.attention.self.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.attention.self.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.attention.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.attention.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.1.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.attention.self.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.attention.self.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.attention.self.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.attention.self.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.attention.self.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.attention.self.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.attention.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.attention.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.2.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.attention.self.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.attention.self.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.attention.self.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.attention.self.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.attention.self.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.attention.self.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.attention.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.attention.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.3.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.attention.self.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.attention.self.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.attention.self.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.attention.self.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.attention.self.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.attention.self.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.attention.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.attention.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.4.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.attention.self.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.attention.self.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.attention.self.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.attention.self.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.attention.self.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.attention.self.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.attention.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.attention.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.5.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.attention.self.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.attention.self.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.attention.self.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.attention.self.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.attention.self.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.attention.self.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.attention.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.attention.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.6.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.attention.self.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.attention.self.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.attention.self.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.attention.self.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.attention.self.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.attention.self.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.attention.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.attention.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.7.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.attention.self.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.attention.self.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.attention.self.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.attention.self.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.attention.self.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.attention.self.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.attention.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.attention.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.8.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.attention.self.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.attention.self.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.attention.self.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.attention.self.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.attention.self.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.attention.self.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.attention.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.attention.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.9.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.attention.self.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.attention.self.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.attention.self.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.attention.self.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.attention.self.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.attention.self.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.attention.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.attention.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.10.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.attention.self.query.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.attention.self.query.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.attention.self.key.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.attention.self.key.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.attention.self.value.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.attention.self.value.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.attention.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.attention.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.attention.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.attention.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.intermediate.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.intermediate.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.output.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.output.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.output.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for bert.encoder.layer.11.output.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for cls.predictions.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for cls.predictions.transform.dense.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for cls.predictions.transform.dense.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for cls.predictions.transform.LayerNorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2068: UserWarning: for cls.predictions.transform.LayerNorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "Refined Prompt (literal): a woman standing on a mountain with a national flag\n",
            "2024-09-08 13:47:00.547 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 590, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 155, in <module>\n",
            "    main()\n",
            "  File \"/content/app.py\", line 147, in main\n",
            "    image = generate_image_with_nlu(prompt, mode=mode.lower(), model_type=model_type.lower())\n",
            "  File \"/content/app.py\", line 111, in generate_image_with_nlu\n",
            "    image = base(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py\", line 1077, in __call__\n",
            "    ) = self.encode_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py\", line 397, in encode_prompt\n",
            "    prompt_embeds = text_encoder(text_input_ids.to(device), output_hidden_states=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 986, in forward\n",
            "    return self.text_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 890, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 813, in forward\n",
            "    layer_outputs = encoder_layer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py\", line 557, in forward\n",
            "    hidden_states = self.layer_norm2(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\", line 202, in forward\n",
            "    return F.layer_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2576, in layer_norm\n",
            "    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n",
            "RuntimeError: expected scalar type Half but found Float\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}